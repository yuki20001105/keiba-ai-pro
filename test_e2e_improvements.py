"""
ã‚¨ãƒ³ãƒ‰ãƒ„ãƒ¼ã‚¨ãƒ³ãƒ‰ãƒ†ã‚¹ãƒˆ: ãƒ‡ãƒ¼ã‚¿åé›† â†’ å­¦ç¿’ â†’ äºˆæ¸¬
æ”¹å–„ã•ã‚ŒãŸã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°æ©Ÿèƒ½ã®çµ±åˆç¢ºèª
"""
import requests
import json
import time

print("\n" + "="*80)
print("  ã€Ultimateç‰ˆã€‘ã‚¨ãƒ³ãƒ‰ãƒ„ãƒ¼ã‚¨ãƒ³ãƒ‰ãƒ†ã‚¹ãƒˆ")
print("="*80)

# ãƒ†ã‚¹ãƒˆè¨­å®š
ULTIMATE_SERVICE_PORT = 8001
TEST_RACE_ID = "202305010101"

def test_step_1_data_collection():
    """ã‚¹ãƒ†ãƒƒãƒ—1: ãƒ‡ãƒ¼ã‚¿åé›†ï¼ˆæ”¹å–„ã•ã‚ŒãŸç‰¹å¾´é‡ã‚’å«ã‚€ï¼‰"""
    print("\nã€ã‚¹ãƒ†ãƒƒãƒ—1: ãƒ‡ãƒ¼ã‚¿åé›†ã€‘")
    print("-" * 80)
    
    try:
        print(f"  Race ID: {TEST_RACE_ID}")
        print(f"  include_details: True (è¿‘èµ°ãƒ‡ãƒ¼ã‚¿æ´¾ç”Ÿç‰¹å¾´ã‚’å«ã‚€)")
        print(f"  include_shutuba: False")
        print("  å®Ÿè¡Œä¸­...")
        
        response = requests.post(
            f"http://localhost:{ULTIMATE_SERVICE_PORT}/scrape/ultimate",
            json={
                "race_id": TEST_RACE_ID,
                "include_details": True,  # è¿‘èµ°ãƒ‡ãƒ¼ã‚¿æ´¾ç”Ÿç‰¹å¾´ã‚’å–å¾—
                "include_shutuba": False
            },
            timeout=120
        )
        
        if response.status_code != 200:
            print(f"  âœ— ã‚¨ãƒ©ãƒ¼: HTTP {response.status_code}")
            return None
        
        data = response.json()
        
        if not data.get('success'):
            print(f"  âœ— ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°å¤±æ•—: {data.get('error')}")
            return None
        
        print(f"  âœ“ å–å¾—æˆåŠŸï¼")
        print(f"    - é ­æ•°: {len(data['results'])}é ­")
        
        # æ”¹å–„ã•ã‚ŒãŸç‰¹å¾´é‡ã®ç¢ºèª
        if len(data['results']) > 0:
            first_horse = data['results'][0]
            print(f"\n  ã€ã‚µãƒ³ãƒ—ãƒ«é¦¬: {first_horse['horse_name']}ã€‘")
            
            features_check = {
                "âœ… æ€§åˆ¥": first_horse.get('sex', 'N/A'),
                "âœ… å¹´é½¢": first_horse.get('age', 'N/A'),
                "âœ… ã‚³ãƒ¼ãƒŠãƒ¼é€šéé †ï¼ˆé…åˆ—ï¼‰": first_horse.get('corner_positions_list', 'N/A'),
                "âœ… ä¸ŠãŒã‚Š3Fé †ä½": first_horse.get('last_3f_rank', 'N/A'),
            }
            
            for key, value in features_check.items():
                print(f"    {key}: {value}")
            
            # è¿‘èµ°æ´¾ç”Ÿç‰¹å¾´ï¼ˆinclude_details=Trueã®å ´åˆï¼‰
            past_features = first_horse.get('past_performance_features', {})
            if past_features:
                print(f"\n  ã€è¿‘èµ°æ´¾ç”Ÿç‰¹å¾´ã€‘")
                print(f"    - å‰èµ°ã‹ã‚‰ã®æ—¥æ•°: {past_features.get('days_since_last_race', 'N/A')}")
                print(f"    - è·é›¢å¤‰åŒ–: {past_features.get('last_distance_change', 'N/A')}")
                print(f"    - äººæ°—ãƒˆãƒ¬ãƒ³ãƒ‰: {past_features.get('popularity_trend', 'N/A')}")
        
        # ãƒ¬ãƒ¼ã‚¹æƒ…å ±
        print(f"\n  ã€ãƒ¬ãƒ¼ã‚¹æƒ…å ±ã€‘")
        print(f"    - ãƒšãƒ¼ã‚¹åŒºåˆ†: {data['race_info'].get('pace_classification', 'N/A')}")
        print(f"    - è·é›¢: {data['race_info'].get('distance', 'N/A')}m")
        print(f"    - ãƒˆãƒ©ãƒƒã‚¯: {data['race_info'].get('track_type', 'N/A')}")
        
        # æ´¾ç”Ÿç‰¹å¾´
        derived = data.get('derived_features', {})
        if derived:
            print(f"\n  ã€æ´¾ç”Ÿç‰¹å¾´ã€‘")
            if 'pace_diff' in derived:
                print(f"    - ãƒšãƒ¼ã‚¹å·®åˆ†: {derived['pace_diff']:.2f}")
            if 'market_entropy' in derived:
                print(f"    - ãƒãƒ¼ã‚±ãƒƒãƒˆã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼: {derived['market_entropy']:.3f}")
        
        return data
        
    except requests.exceptions.Timeout:
        print(f"  âœ— ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ: ã‚µãƒ¼ãƒ“ã‚¹ãŒå¿œç­”ã—ã¾ã›ã‚“")
        return None
    except Exception as e:
        print(f"  âœ— ã‚¨ãƒ©ãƒ¼: {e}")
        import traceback
        traceback.print_exc()
        return None


def test_step_2_feature_validation(scrape_data):
    """ã‚¹ãƒ†ãƒƒãƒ—2: ç‰¹å¾´é‡ã®æ¤œè¨¼"""
    print("\nã€ã‚¹ãƒ†ãƒƒãƒ—2: ç‰¹å¾´é‡æ¤œè¨¼ã€‘")
    print("-" * 80)
    
    if not scrape_data or not scrape_data.get('success'):
        print("  âœ— ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ãƒ‡ãƒ¼ã‚¿ãªã—")
        return False
    
    results = scrape_data.get('results', [])
    if len(results) == 0:
        print("  âœ— çµæœãƒ‡ãƒ¼ã‚¿ãªã—")
        return False
    
    # æ–°æ©Ÿèƒ½ã®æ¤œè¨¼
    checks = []
    
    # 1. æ€§é½¢ãƒ‘ãƒ¼ã‚¹
    sex_parsed = all('sex' in h and 'age' in h for h in results)
    checks.append(("æ€§é½¢ãƒ‘ãƒ¼ã‚¹", sex_parsed))
    
    # 2. ã‚³ãƒ¼ãƒŠãƒ¼é€šéé †é…åˆ—åŒ–
    corner_parsed = all('corner_positions_list' in h for h in results)
    checks.append(("ã‚³ãƒ¼ãƒŠãƒ¼é€šéé †é…åˆ—åŒ–", corner_parsed))
    
    # 3. ä¸ŠãŒã‚Šé †ä½
    rank_calculated = all('last_3f_rank' in h for h in results)
    checks.append(("ä¸ŠãŒã‚Š3Fé †ä½è¨ˆç®—", rank_calculated))
    
    # 4. ãƒšãƒ¼ã‚¹åŒºåˆ†
    pace_exists = 'pace_classification' in scrape_data.get('race_info', {})
    checks.append(("ãƒšãƒ¼ã‚¹åŒºåˆ†å–å¾—", pace_exists))
    
    # 5. æ´¾ç”Ÿç‰¹å¾´
    derived_exists = len(scrape_data.get('derived_features', {})) > 0
    checks.append(("æ´¾ç”Ÿç‰¹å¾´è¨ˆç®—", derived_exists))
    
    print("  ã€æ¤œè¨¼çµæœã€‘")
    all_passed = True
    for name, passed in checks:
        status = "âœ“" if passed else "âœ—"
        print(f"    {status} {name}: {'åˆæ ¼' if passed else 'å¤±æ•—'}")
        if not passed:
            all_passed = False
    
    return all_passed


def test_step_3_ml_compatibility():
    """ã‚¹ãƒ†ãƒƒãƒ—3: æ©Ÿæ¢°å­¦ç¿’äº’æ›æ€§ãƒ†ã‚¹ãƒˆ"""
    print("\nã€ã‚¹ãƒ†ãƒƒãƒ—3: æ©Ÿæ¢°å­¦ç¿’äº’æ›æ€§ã€‘")
    print("-" * 80)
    
    print("  ç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°é–¢æ•°ã®ãƒ†ã‚¹ãƒˆ...")
    
    try:
        import sys
        sys.path.insert(0, r"C:\Users\yuki2\Documents\ws\keiba-ai-pro")
        sys.path.insert(0, r"C:\Users\yuki2\Documents\ws\keiba-ai-pro\keiba")
        
        from keiba_ai.feature_engineering import add_derived_features
        import pandas as pd
        import numpy as np
        
        # ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ä½œæˆ
        test_df = pd.DataFrame({
            'race_id': ['202305010101'] * 3,
            'horse_name': ['é¦¬A', 'é¦¬B', 'é¦¬C'],
            'sex': ['ç‰¡', 'ç‰', 'ç‰¡'],
            'age': [3, 4, 5],
            'corner_positions_list': [[1, 2, 1, 1], [5, 4, 3, 2], [3, 3, 3, 3]],
            'last_3f_rank': [1, 2, 3],
            'days_since_last_race': [14, 30, 60],
            'last_distance_change': [200, -200, 0],
            'popularity_trend': ['improving', 'declining', 'stable'],
            'pace_classification': ['H', 'M', 'S'],
            'num_horses': [16, 16, 16],
            'distance': [1400, 1400, 1400],
            'surface': ['turf', 'turf', 'turf']
        })
        
        print(f"    å…¥åŠ›ãƒ‡ãƒ¼ã‚¿: {len(test_df)} è¡Œ")
        
        # ç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°å®Ÿè¡Œ
        result_df = add_derived_features(test_df)
        
        print(f"    å‡ºåŠ›ãƒ‡ãƒ¼ã‚¿: {len(result_df)} è¡Œ, {len(result_df.columns)} åˆ—")
        
        # æ–°æ©Ÿèƒ½ã«ã‚ˆã‚‹è¿½åŠ ç‰¹å¾´é‡ã‚’ç¢ºèª
        new_features = []
        if 'sex_ç‰¡' in result_df.columns:
            new_features.append('æ€§åˆ¥ãƒ€ãƒŸãƒ¼å¤‰æ•°')
        if 'is_young' in result_df.columns:
            new_features.append('å¹´é½¢ã‚«ãƒ†ã‚´ãƒª')
        if 'corner_position_avg' in result_df.columns:
            new_features.append('ã‚³ãƒ¼ãƒŠãƒ¼å¹³å‡ä½ç½®')
        if 'position_change' in result_df.columns:
            new_features.append('ãƒã‚¸ã‚·ãƒ§ãƒ³å¤‰åŒ–')
        if 'pace_H' in result_df.columns or 'pace_M' in result_df.columns:
            new_features.append('ãƒšãƒ¼ã‚¹ãƒ€ãƒŸãƒ¼å¤‰æ•°')
        if 'rest_short' in result_df.columns:
            new_features.append('ä¼‘é¤ŠæœŸé–“ã‚«ãƒ†ã‚´ãƒª')
        if 'distance_increased' in result_df.columns:
            new_features.append('è·é›¢å¤‰åŒ–ãƒ•ãƒ©ã‚°')
        
        print(f"\n  ã€ç”Ÿæˆã•ã‚ŒãŸæ–°ç‰¹å¾´é‡ã€‘")
        for feature in new_features:
            print(f"    âœ“ {feature}")
        
        print(f"\n  âœ“ ç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°æˆåŠŸï¼")
        print(f"    å…ƒã®ã‚«ãƒ©ãƒ æ•°: {len(test_df.columns)}")
        print(f"    å‡¦ç†å¾Œã‚«ãƒ©ãƒ æ•°: {len(result_df.columns)}")
        print(f"    è¿½åŠ ã•ã‚ŒãŸç‰¹å¾´: {len(result_df.columns) - len(test_df.columns)}å€‹")
        
        return True
        
    except Exception as e:
        print(f"  âœ— ã‚¨ãƒ©ãƒ¼: {e}")
        import traceback
        traceback.print_exc()
        return False


def main():
    print("\nğŸ¯ æ”¹å–„ã•ã‚ŒãŸã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°æ©Ÿèƒ½ã®çµ±åˆãƒ†ã‚¹ãƒˆã‚’é–‹å§‹ã—ã¾ã™\n")
    
    # ã‚¹ãƒ†ãƒƒãƒ—1: ãƒ‡ãƒ¼ã‚¿åé›†
    scrape_data = test_step_1_data_collection()
    
    if scrape_data:
        time.sleep(1)
        
        # ã‚¹ãƒ†ãƒƒãƒ—2: ç‰¹å¾´é‡æ¤œè¨¼
        features_valid = test_step_2_feature_validation(scrape_data)
        
        time.sleep(1)
        
        # ã‚¹ãƒ†ãƒƒãƒ—3: æ©Ÿæ¢°å­¦ç¿’äº’æ›æ€§
        ml_compatible = test_step_3_ml_compatibility()
        
        # ç·åˆè©•ä¾¡
        print("\n" + "="*80)
        print("  ã€ç·åˆè©•ä¾¡ã€‘")
        print("="*80)
        
        if scrape_data and features_valid and ml_compatible:
            print("\n  âœ… ã™ã¹ã¦ã®ãƒ†ã‚¹ãƒˆã«åˆæ ¼ã—ã¾ã—ãŸï¼")
            print("\n  ã€æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—ã€‘")
            print("    1. ãƒ‡ãƒ¼ã‚¿åé›†UIã§å®Ÿéš›ã«ãƒ‡ãƒ¼ã‚¿ã‚’åé›†")
            print("    2. å­¦ç¿’æ©Ÿèƒ½ã§æ–°ã—ã„ç‰¹å¾´é‡ã‚’ä½¿ã£ã¦ãƒ¢ãƒ‡ãƒ«å­¦ç¿’")
            print("    3. äºˆæ¸¬æ©Ÿèƒ½ã§å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ã£ã¦äºˆæ¸¬")
        else:
            print("\n  âš ï¸ ä¸€éƒ¨ã®ãƒ†ã‚¹ãƒˆãŒå¤±æ•—ã—ã¾ã—ãŸ")
            print("    - ãƒ‡ãƒ¼ã‚¿åé›†: ", "âœ“" if scrape_data else "âœ—")
            print("    - ç‰¹å¾´é‡æ¤œè¨¼: ", "âœ“" if features_valid else "âœ—")
            print("    - MLäº’æ›æ€§: ", "âœ“" if ml_compatible else "âœ—")
    else:
        print("\n  âœ— ãƒ‡ãƒ¼ã‚¿åé›†ã«å¤±æ•—ã—ãŸãŸã‚ã€ãƒ†ã‚¹ãƒˆã‚’ä¸­æ–­ã—ã¾ã—ãŸ")
        print("\n  ã€ç¢ºèªäº‹é …ã€‘")
        print("    - Ultimateã‚µãƒ¼ãƒ“ã‚¹ãŒèµ·å‹•ã—ã¦ã„ã¾ã™ã‹ï¼Ÿ(port 8001)")
        print("    - race_id ã¯æ­£ã—ã„ã§ã™ã‹ï¼Ÿ")
    
    print("\n" + "="*80 + "\n")


if __name__ == "__main__":
    main()
