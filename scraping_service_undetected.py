"""
undetected-chromedriver + ãƒ¬ãƒ¼ãƒˆåˆ¶é™æ©Ÿèƒ½ä»˜ãã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã‚µãƒ¼ãƒ“ã‚¹
é€šå¸¸IPå„ªå…ˆã€ãƒ–ãƒ­ãƒƒã‚¯æ™‚ã¯VPNæ¨å¥¨
"""
from fastapi import FastAPI, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
import undetected_chromedriver as uc
from bs4 import BeautifulSoup
import requests
import time
import random
from datetime import datetime
from typing import Optional
import threading

app = FastAPI()

# CORSè¨­å®š
app.add_middleware(
    CORSMiddleware,
    allow_origins=["http://localhost:3000"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# ãƒ¬ãƒ¼ãƒˆåˆ¶é™ç®¡ç†
class RateLimiter:
    def __init__(self, min_interval=3.0, max_interval=7.0):
        self.min_interval = min_interval
        self.max_interval = max_interval
        self.last_request_time: Optional[datetime] = None
        self.request_count = 0
        self.start_time = datetime.now()
        self.lock = threading.Lock()
    
    def wait_if_needed(self):
        """å¿…è¦ã«å¿œã˜ã¦å¾…æ©Ÿ"""
        with self.lock:
            if self.last_request_time is None:
                self.last_request_time = datetime.now()
                return 0
            
            elapsed = (datetime.now() - self.last_request_time).total_seconds()
            required_wait = random.uniform(self.min_interval, self.max_interval)
            
            if elapsed < required_wait:
                wait_time = required_wait - elapsed
                print(f"â° ãƒ¬ãƒ¼ãƒˆåˆ¶é™: {wait_time:.1f}ç§’å¾…æ©Ÿã—ã¾ã™...")
                time.sleep(wait_time)
            else:
                wait_time = 0
            
            self.last_request_time = datetime.now()
            self.request_count += 1
            
            total_elapsed = (datetime.now() - self.start_time).total_seconds()
            avg_interval = total_elapsed / self.request_count if self.request_count > 0 else 0
            print(f"ğŸ“Š ãƒªã‚¯ã‚¨ã‚¹ãƒˆçµ±è¨ˆ: {self.request_count}å›, å¹³å‡é–“éš”: {avg_interval:.1f}ç§’")
            
            return wait_time

# ã‚°ãƒ­ãƒ¼ãƒãƒ«ãªãƒ¬ãƒ¼ãƒˆåˆ¶é™ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹
rate_limiter = RateLimiter(min_interval=3.0, max_interval=7.0)

# ã‚°ãƒ­ãƒ¼ãƒãƒ«ãªChromeãƒ‰ãƒ©ã‚¤ãƒãƒ¼ï¼ˆå†åˆ©ç”¨ã§é«˜é€ŸåŒ–ï¼‰
_driver: Optional[uc.Chrome] = None
_driver_lock = threading.Lock()

def get_driver():
    """Chromeãƒ‰ãƒ©ã‚¤ãƒãƒ¼ã‚’å–å¾—ï¼ˆã‚·ãƒ³ã‚°ãƒ«ãƒˆãƒ³ãƒ‘ã‚¿ãƒ¼ãƒ³ï¼‰"""
    global _driver
    with _driver_lock:
        if _driver is None:
            print("ğŸš€ Chrome WebDriveråˆæœŸåŒ–ä¸­...")
            options = uc.ChromeOptions()
            options.headless = False  # éãƒ˜ãƒƒãƒ‰ãƒ¬ã‚¹ãƒ¢ãƒ¼ãƒ‰ï¼ˆå®‰å®šæ€§å„ªå…ˆï¼‰
            options.add_argument('--no-sandbox')
            options.add_argument('--disable-dev-shm-usage')
            _driver = uc.Chrome(options=options, use_subprocess=False, version_main=None)
            print("âœ“ Chrome WebDriveråˆæœŸåŒ–å®Œäº†")
        return _driver

class ScrapeRequest(BaseModel):
    race_id: str

class RaceListRequest(BaseModel):
    kaisai_date: str  # YYYYMMDDå½¢å¼

class RaceListResponse(BaseModel):
    success: bool
    race_ids: list[str] = []
    error: str | None = None

class ScrapeResponse(BaseModel):
    success: bool
    race_name: str | None = None
    race_data: str | None = None
    distance: int | None = None
    track_type: str | None = None
    weather: str | None = None
    field_condition: str | None = None
    results: list[dict] = []
    payouts: list[dict] = []
    error: str | None = None
    wait_time: float | None = None


def check_ip_blocked():
    """ç¾åœ¨ã®IPãŒãƒ–ãƒ­ãƒƒã‚¯ã•ã‚Œã¦ã„ã‚‹ã‹ãƒã‚§ãƒƒã‚¯ï¼ˆæ³¨æ„ï¼šundetected-chromedriverãªã‚‰å›é¿å¯èƒ½ï¼‰"""
    try:
        # é€šå¸¸ã®requestsã§è»½é‡ãƒã‚§ãƒƒã‚¯
        test_response = requests.get(
            'https://race.netkeiba.com/',
            headers={'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'},
            timeout=10
        )
        
        # 400ã‚¨ãƒ©ãƒ¼ã®å ´åˆã¯ç¢ºå®Ÿã«ãƒ–ãƒ­ãƒƒã‚¯
        if test_response.status_code == 400:
            return True
        # 50ãƒã‚¤ãƒˆç¨‹åº¦ã®å°ã•ãªãƒ¬ã‚¹ãƒãƒ³ã‚¹ã¯è­¦å‘Šã®ã¿ï¼ˆundetected-chromedriverãªã‚‰å›é¿å¯èƒ½ï¼‰
        if len(test_response.content) < 10000:
            print("  âš  é€šå¸¸ã®requestsã§ã¯ãƒ–ãƒ­ãƒƒã‚¯ã•ã‚Œã¦ã„ã¾ã™ãŒã€undetected-chromedriverã§è©¦è¡Œã—ã¾ã™")
            return False
        return False
        
    except Exception as e:
        # ã‚¨ãƒ©ãƒ¼ã®å ´åˆã‚‚è©¦è¡Œã™ã‚‹ï¼ˆundetected-chromedriverã§å›é¿ã§ãã‚‹å¯èƒ½æ€§ã‚ã‚Šï¼‰
        print(f"  âš  IPçŠ¶æ…‹ãƒã‚§ãƒƒã‚¯ã‚¨ãƒ©ãƒ¼ï¼ˆ{type(e).__name__}ï¼‰ã€undetected-chromedriverã§è©¦è¡Œã—ã¾ã™")
        return False

@app.post("/scrape/race", response_model=ScrapeResponse)
def scrape_race(request: ScrapeRequest):
    """
    ãƒ¬ãƒ¼ã‚¹çµæœã‚’ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ï¼ˆundetected-chromedriverä½¿ç”¨ï¼‰
    - åˆå›ãƒªã‚¯ã‚¨ã‚¹ãƒˆæ™‚ã«IPçŠ¶æ…‹ã‚’ãƒã‚§ãƒƒã‚¯ï¼ˆ400ã‚¨ãƒ©ãƒ¼ã®å ´åˆã®ã¿VPNæ¨å¥¨ï¼‰
    - ãã‚Œä»¥å¤–ã¯undetected-chromedriverã§å–å¾—ã‚’è©¦è¡Œ
    """
    # ãƒ¬ãƒ¼ãƒˆåˆ¶é™ãƒã‚§ãƒƒã‚¯
    wait_time = rate_limiter.wait_if_needed()
    
    race_id = request.race_id
    url = f'https://race.netkeiba.com/race/result.html?race_id={race_id}'
    
    # IPçŠ¶æ…‹ãƒã‚§ãƒƒã‚¯ï¼ˆåˆå›ãƒªã‚¯ã‚¨ã‚¹ãƒˆæ™‚ã®ã¿ã€400ã‚¨ãƒ©ãƒ¼ã®å ´åˆã®ã¿ãƒ–ãƒ­ãƒƒã‚¯åˆ¤å®šï¼‰
    if rate_limiter.request_count == 1:
        print("â†’ IPçŠ¶æ…‹ãƒã‚§ãƒƒã‚¯ä¸­...")
        if check_ip_blocked():
            print("âœ— ç¢ºå®Ÿã«ãƒ–ãƒ­ãƒƒã‚¯ã•ã‚Œã¦ã„ã¾ã™ï¼ˆ400ã‚¨ãƒ©ãƒ¼ï¼‰")
            return ScrapeResponse(
                success=False,
                error="IPã‚¢ãƒ‰ãƒ¬ã‚¹ãŒå®Œå…¨ã«ãƒ–ãƒ­ãƒƒã‚¯ã•ã‚Œã¦ã„ã¾ã™ï¼ˆ400ã‚¨ãƒ©ãƒ¼ï¼‰ã€‚ProtonVPNç­‰ã®VPNã«æ¥ç¶šã—ã¦ã‹ã‚‰å†åº¦ãŠè©¦ã—ãã ã•ã„ã€‚",
                wait_time=wait_time
            )
        else:
            print("âœ“ é€šå¸¸IPã§ã‚¢ã‚¯ã‚»ã‚¹å¯èƒ½")
    
    try:
        # Chromeãƒ‰ãƒ©ã‚¤ãƒãƒ¼ã‚’å–å¾—
        driver = get_driver()
        
        print(f"â†’ Opening URL: {url}")
        
        # äººé–“ã‚‰ã—ã„é…å»¶
        time.sleep(random.uniform(1.5, 3.0))
        
        # ãƒšãƒ¼ã‚¸ã‚’é–‹ã
        driver.get(url)
        
        # JavaScriptãƒ¬ãƒ³ãƒ€ãƒªãƒ³ã‚°å¾…æ©Ÿ
        time.sleep(random.uniform(2.0, 4.0))
        
        # ãƒšãƒ¼ã‚¸ã‚¿ã‚¤ãƒˆãƒ«ã‚’ç¢ºèª
        title = driver.title
        print(f"âœ“ Page loaded: {title}")
        
        # HTMLã‚’å–å¾—
        html = driver.page_source
        content_length = len(html)
        print(f"âœ“ HTML retrieved: {content_length:,} bytes")
        
        # ã‚¨ãƒ©ãƒ¼ãƒšãƒ¼ã‚¸ã®ãƒã‚§ãƒƒã‚¯
        if content_length < 10000:
            return ScrapeResponse(
                success=False,
                error="ãƒšãƒ¼ã‚¸ãŒæ­£å¸¸ã«èª­ã¿è¾¼ã¾ã‚Œã¾ã›ã‚“ã§ã—ãŸã€‚race_idã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚",
                wait_time=wait_time
            )
        
        # BeautifulSoupã§ãƒ‘ãƒ¼ã‚¹
        soup = BeautifulSoup(html, 'html.parser')
        
        # ãƒ¬ãƒ¼ã‚¹å
        race_name_elem = soup.find('h1', class_='RaceName')
        if not race_name_elem:
            # å‡ºé¦¬è¡¨ãƒšãƒ¼ã‚¸ã®å¯èƒ½æ€§ã‚‚ãƒã‚§ãƒƒã‚¯
            race_name_elem = soup.find('div', class_='RaceName')
        
        if not race_name_elem:
            return ScrapeResponse(
                success=False,
                error='ãƒ¬ãƒ¼ã‚¹åãŒå–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸã€‚race_idãŒæ­£ã—ã„ã‹ç¢ºèªã—ã¦ãã ã•ã„ã€‚',
                wait_time=wait_time
            )
        
        race_name = race_name_elem.text.strip()
        print(f"âœ“ ãƒ¬ãƒ¼ã‚¹å: {race_name}")
        
        # ãƒ¬ãƒ¼ã‚¹ãƒ‡ãƒ¼ã‚¿
        race_data_elem = soup.find('div', class_='RaceData01')
        if not race_data_elem:
            race_data_elem = soup.find('div', class_='RaceData02')
        
        race_data_text = race_data_elem.text.strip() if race_data_elem else ''
        
        # è·é›¢ãƒ»ãƒˆãƒ©ãƒƒã‚¯ç¨®åˆ¥ãªã©ã‚’æŠ½å‡º
        distance = None
        track_type = ''
        weather = ''
        field_condition = ''
        
        if race_data_text:
            import re
            # è·é›¢
            dist_match = re.search(r'(\d+)m', race_data_text)
            if dist_match:
                distance = int(dist_match.group(1))
            
            # ãƒˆãƒ©ãƒƒã‚¯ç¨®åˆ¥
            if 'èŠ' in race_data_text:
                track_type = 'èŠ'
            elif 'ãƒ€ãƒ¼ãƒˆ' in race_data_text or 'ãƒ€' in race_data_text:
                track_type = 'ãƒ€ãƒ¼ãƒˆ'
            
            # å¤©å€™
            weather_match = re.search(r'å¤©å€™:([^/\s]+)', race_data_text)
            if weather_match:
                weather = weather_match.group(1).strip()
            
            # é¦¬å ´çŠ¶æ…‹
            field_match = re.search(r'é¦¬å ´:([^/\s]+)', race_data_text)
            if field_match:
                field_condition = field_match.group(1).strip()
        
        # çµæœãƒ†ãƒ¼ãƒ–ãƒ«
        results = []
        result_table = soup.find('table', class_='Result_Table')
        if result_table:
            rows = result_table.find_all('tr')
            for row in rows[1:]:
                cols = row.find_all('td')
                if len(cols) >= 10:
                    result = {
                        'finish_position': cols[0].text.strip(),
                        'bracket_number': cols[1].text.strip(),
                        'horse_number': cols[2].text.strip(),
                        'horse_name': cols[3].text.strip(),
                        'sex_age': cols[4].text.strip(),
                        'jockey_weight': cols[5].text.strip(),
                        'jockey_name': cols[6].text.strip(),
                        'finish_time': cols[7].text.strip(),
                        'margin': cols[8].text.strip(),
                        'odds': cols[9].text.strip(),
                    }
                    results.append(result)
        
        print(f"âœ“ çµæœ: {len(results)}é ­åˆ†ã®ãƒ‡ãƒ¼ã‚¿")
        
        # æ‰•ã„æˆ»ã—ãƒ†ãƒ¼ãƒ–ãƒ«
        payouts = []
        payout_table = soup.find('table', class_='Payout_Detail_Table')
        if payout_table:
            rows = payout_table.find_all('tr')
            for row in rows:
                cols = row.find_all('td')
                if len(cols) >= 3:
                    payout = {
                        'type': cols[0].text.strip(),
                        'numbers': cols[1].text.strip(),
                        'amount': cols[2].text.strip(),
                    }
                    payouts.append(payout)
        
        print(f"âœ“ æ‰•ã„æˆ»ã—: {len(payouts)}ä»¶")
        
        return ScrapeResponse(
            success=True,
            race_name=race_name,
            race_data=race_data_text,
            distance=distance,
            track_type=track_type,
            weather=weather,
            field_condition=field_condition,
            results=results,
            payouts=payouts,
            wait_time=wait_time
        )
        
    except Exception as e:
        print(f"âœ— ã‚¨ãƒ©ãƒ¼: {type(e).__name__}: {str(e)}")
        return ScrapeResponse(success=False, error=str(e), wait_time=wait_time)


@app.get("/health")
def health_check():
    """ãƒ˜ãƒ«ã‚¹ãƒã‚§ãƒƒã‚¯"""
    return {
        "status": "ok",
        "request_count": rate_limiter.request_count,
        "uptime_seconds": (datetime.now() - rate_limiter.start_time).total_seconds(),
        "driver_initialized": _driver is not None
    }


@app.get("/stats")
def get_stats():
    """çµ±è¨ˆæƒ…å ±"""
    total_elapsed = (datetime.now() - rate_limiter.start_time).total_seconds()
    avg_interval = total_elapsed / rate_limiter.request_count if rate_limiter.request_count > 0 else 0
    
    return {
        "total_requests": rate_limiter.request_count,
        "uptime_seconds": total_elapsed,
        "average_interval_seconds": avg_interval,
        "rate_limit_config": {
            "min_interval": rate_limiter.min_interval,
            "max_interval": rate_limiter.max_interval
        },
        "driver_status": "initialized" if _driver is not None else "not initialized"
    }


@app.post("/race_list", response_model=RaceListResponse)
def get_race_list(request: RaceListRequest):
    """
    æŒ‡å®šæ—¥ã®race_idä¸€è¦§ã‚’å–å¾—
    race_list.html?kaisai_date=YYYYMMDDã‹ã‚‰å®Ÿéš›ã®race_idã‚’å–å¾—
    """
    kaisai_date = request.kaisai_date
    url = f'https://race.netkeiba.com/top/race_list.html?kaisai_date={kaisai_date}'
    
    print(f"ğŸ“… {kaisai_date[:4]}å¹´{kaisai_date[4:6]}æœˆ{kaisai_date[6:8]}æ—¥ã®ãƒ¬ãƒ¼ã‚¹ä¸€è¦§å–å¾—ä¸­...")
    
    try:
        driver = get_driver()
        driver.get(url)
        
        # ãƒšãƒ¼ã‚¸èª­ã¿è¾¼ã¿å¾…æ©Ÿ
        time.sleep(random.uniform(2.0, 3.0))
        
        html = driver.page_source
        soup = BeautifulSoup(html, 'html.parser')
        
        # race_idã‚’æŠ½å‡º
        import re
        race_ids = []
        for link in soup.find_all('a', href=True):
            href = link['href']
            match = re.search(r'race_id=(\d{12})', href)
            if match:
                race_id = match.group(1)
                if race_id not in race_ids:
                    race_ids.append(race_id)
        
        print(f"âœ“ {len(race_ids)}ä»¶ã®ãƒ¬ãƒ¼ã‚¹ã‚’å–å¾—")
        
        return RaceListResponse(
            success=True,
            race_ids=race_ids
        )
        
    except Exception as e:
        print(f"âœ— ã‚¨ãƒ©ãƒ¼: {e}")
        return RaceListResponse(
            success=False,
            error=str(e)
        )


@app.on_event("shutdown")
def shutdown_event():
    """ã‚·ãƒ£ãƒƒãƒˆãƒ€ã‚¦ãƒ³æ™‚ã«ãƒ‰ãƒ©ã‚¤ãƒãƒ¼ã‚’ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—"""
    global _driver
    if _driver is not None:
        print("ğŸ›‘ Chrome WebDriverã‚’ã‚¯ãƒ­ãƒ¼ã‚ºã—ã¾ã™...")
        _driver.quit()
        _driver = None


if __name__ == '__main__':
    import uvicorn
    print("=" * 80)
    print("undetected-chromedriver + ãƒ¬ãƒ¼ãƒˆåˆ¶é™æ©Ÿèƒ½ä»˜ãã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã‚µãƒ¼ãƒ“ã‚¹èµ·å‹•")
    print("=" * 80)
    print(f"æœ€å°é–“éš”: {rate_limiter.min_interval}ç§’")
    print(f"æœ€å¤§é–“éš”: {rate_limiter.max_interval}ç§’")
    print(f"Botå›é¿: undetected-chromedriverä½¿ç”¨")
    print("=" * 80)
    uvicorn.run(app, host='0.0.0.0', port=8001)
