"""
ãƒ¬ãƒ¼ãƒˆåˆ¶é™æ©Ÿèƒ½ä»˜ãã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã‚µãƒ¼ãƒ“ã‚¹
å„ãƒªã‚¯ã‚¨ã‚¹ãƒˆé–“ã«3ã€œ7ç§’ã®é–“éš”ã‚’ç¢ºä¿ã—ã¦IPãƒ–ãƒ­ãƒƒã‚¯ã‚’é˜²ã
"""
from fastapi import FastAPI, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from webdriver_manager.chrome import ChromeDriverManager
from bs4 import BeautifulSoup
import time
import random
from datetime import datetime, timedelta
from typing import Optional

app = FastAPI()

# CORSè¨­å®š
app.add_middleware(
    CORSMiddleware,
    allow_origins=["http://localhost:3000"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# ãƒ¬ãƒ¼ãƒˆåˆ¶é™ç®¡ç†
class RateLimiter:
    def __init__(self, min_interval=3.0, max_interval=7.0):
        self.min_interval = min_interval  # æœ€å°é–“éš”ï¼ˆç§’ï¼‰
        self.max_interval = max_interval  # æœ€å¤§é–“éš”ï¼ˆç§’ï¼‰
        self.last_request_time: Optional[datetime] = None
        self.request_count = 0
        self.start_time = datetime.now()
    
    def wait_if_needed(self):
        """å¿…è¦ã«å¿œã˜ã¦å¾…æ©Ÿ"""
        if self.last_request_time is None:
            self.last_request_time = datetime.now()
            return
        
        # å‰å›ã®ãƒªã‚¯ã‚¨ã‚¹ãƒˆã‹ã‚‰ã®çµŒéæ™‚é–“
        elapsed = (datetime.now() - self.last_request_time).total_seconds()
        
        # ãƒ©ãƒ³ãƒ€ãƒ ãªå¾…æ©Ÿæ™‚é–“ï¼ˆ3ã€œ7ç§’ï¼‰
        required_wait = random.uniform(self.min_interval, self.max_interval)
        
        if elapsed < required_wait:
            wait_time = required_wait - elapsed
            print(f"â° ãƒ¬ãƒ¼ãƒˆåˆ¶é™: {wait_time:.1f}ç§’å¾…æ©Ÿã—ã¾ã™...")
            time.sleep(wait_time)
        
        self.last_request_time = datetime.now()
        self.request_count += 1
        
        # çµ±è¨ˆæƒ…å ±ã‚’è¡¨ç¤º
        total_elapsed = (datetime.now() - self.start_time).total_seconds()
        avg_interval = total_elapsed / self.request_count if self.request_count > 0 else 0
        print(f"ğŸ“Š ãƒªã‚¯ã‚¨ã‚¹ãƒˆçµ±è¨ˆ: {self.request_count}å›, å¹³å‡é–“éš”: {avg_interval:.1f}ç§’")

# ã‚°ãƒ­ãƒ¼ãƒãƒ«ãªãƒ¬ãƒ¼ãƒˆåˆ¶é™ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹
rate_limiter = RateLimiter(min_interval=3.0, max_interval=7.0)

class ScrapeRequest(BaseModel):
    race_id: str

class ScrapeResponse(BaseModel):
    success: bool
    race_name: str | None = None
    race_data: str | None = None
    distance: int | None = None
    track_type: str | None = None
    weather: str | None = None
    field_condition: str | None = None
    results: list[dict] = []
    payouts: list[dict] = []
    error: str | None = None
    wait_time: float | None = None  # å¾…æ©Ÿæ™‚é–“ï¼ˆç§’ï¼‰


@app.post("/scrape/race", response_model=ScrapeResponse)
def scrape_race(request: ScrapeRequest):
    """
    ãƒ¬ãƒ¼ã‚¹çµæœã‚’ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ï¼ˆãƒ¬ãƒ¼ãƒˆåˆ¶é™ä»˜ãï¼‰
    """
    # ãƒ¬ãƒ¼ãƒˆåˆ¶é™ãƒã‚§ãƒƒã‚¯
    start_time = time.time()
    rate_limiter.wait_if_needed()
    wait_time = time.time() - start_time
    
    race_id = request.race_id
    url = f'https://race.netkeiba.com/race/result.html?race_id={race_id}'
    
    try:
        # Chromeã‚ªãƒ—ã‚·ãƒ§ãƒ³è¨­å®šï¼ˆbotæ¤œå‡ºå›é¿ï¼‰
        chrome_options = Options()
        chrome_options.add_argument('--headless=new')  # æ–°ã—ã„ãƒ˜ãƒƒãƒ‰ãƒ¬ã‚¹ãƒ¢ãƒ¼ãƒ‰
        chrome_options.add_argument('--no-sandbox')
        chrome_options.add_argument('--disable-dev-shm-usage')
        chrome_options.add_argument('--disable-blink-features=AutomationControlled')
        chrome_options.add_experimental_option('excludeSwitches', ['enable-automation'])
        chrome_options.add_experimental_option('useAutomationExtension', False)
        chrome_options.add_argument('user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36')
        
        # WebDriverã‚’åˆæœŸåŒ–
        service = Service(ChromeDriverManager().install())
        driver = webdriver.Chrome(service=service, options=chrome_options)
        
        print(f"âœ“ Chrome WebDriver initialized")
        
        # webdriveræ¤œå‡ºã‚’å›é¿
        driver.execute_script("Object.defineProperty(navigator, 'webdriver', {get: () => undefined})")
        
        try:
            # äººé–“ã‚‰ã—ã„é…å»¶
            time.sleep(random.uniform(1.5, 3.0))
            
            # ãƒšãƒ¼ã‚¸ã‚’é–‹ã
            print(f"â†’ Opening URL: {url}")
            driver.get(url)
            print(f"âœ“ Page loaded: {driver.title}")
            
            # ãƒšãƒ¼ã‚¸ã®URLã‚’ç¢ºèª
            current_url = driver.current_url
            print(f"  Current URL: {current_url}")
            
            # ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ã‚³ãƒ¼ãƒ‰ã®ãƒã‚§ãƒƒã‚¯ï¼ˆJavaScriptã§ç¢ºèªï¼‰
            status_code = driver.execute_script("return document.readyState")
            print(f"  Page state: {status_code}")
            
            # ãƒ¬ãƒ¼ã‚¹åãŒè¡¨ç¤ºã•ã‚Œã‚‹ã¾ã§å¾…æ©Ÿï¼ˆæœ€å¤§15ç§’ï¼‰
            try:
                WebDriverWait(driver, 15).until(
                    EC.presence_of_element_located((By.CSS_SELECTOR, 'h1.RaceName'))
                )
                print(f"âœ“ ãƒ¬ãƒ¼ã‚¹åè¦ç´ ã‚’æ¤œå‡º")
            except Exception as e:
                print(f"âš  ãƒ¬ãƒ¼ã‚¹åè¦ç´ ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {e}")
                # ãƒšãƒ¼ã‚¸ã‚½ãƒ¼ã‚¹ã‚’ç¢ºèª
                page_length = len(driver.page_source)
                print(f"  ãƒšãƒ¼ã‚¸ã‚µã‚¤ã‚º: {page_length} bytes")
                
                if page_length < 1000:
                    # 400ã‚¨ãƒ©ãƒ¼ã¾ãŸã¯ãƒ–ãƒ­ãƒƒã‚¯ã®å¯èƒ½æ€§
                    driver.quit()
                    return ScrapeResponse(
                        success=False,
                        error="ãƒšãƒ¼ã‚¸ãŒæ­£å¸¸ã«èª­ã¿è¾¼ã¾ã‚Œã¾ã›ã‚“ã§ã—ãŸã€‚IPãƒ–ãƒ­ãƒƒã‚¯ã®å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚",
                        wait_time=wait_time
                    )
            
            # è¿½åŠ ã®å¾…æ©Ÿï¼ˆJavaScriptã®å®Œå…¨å®Ÿè¡Œã‚’ç¢ºä¿ï¼‰
            time.sleep(random.uniform(2.0, 4.0))
            
            # HTMLã‚’å–å¾—
            html = driver.page_source
            print(f"âœ“ HTML retrieved: {len(html):,} bytes")
            
        finally:
            # ãƒ–ãƒ©ã‚¦ã‚¶ã‚’é–‰ã˜ã‚‹
            driver.quit()
            print(f"âœ“ Browser closed")
        
        # BeautifulSoupã§ãƒ‘ãƒ¼ã‚¹
        soup = BeautifulSoup(html, 'html.parser')
        
        # ãƒ¬ãƒ¼ã‚¹å
        race_name_elem = soup.find('h1', class_='RaceName')
        if not race_name_elem:
            return ScrapeResponse(
                success=False,
                error='ãƒ¬ãƒ¼ã‚¹åãŒå–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸã€‚race_idãŒæ­£ã—ã„ã‹ç¢ºèªã—ã¦ãã ã•ã„ã€‚',
                wait_time=wait_time
            )
        
        race_name = race_name_elem.text.strip()
        print(f"âœ“ ãƒ¬ãƒ¼ã‚¹å: {race_name}")
        
        # ãƒ¬ãƒ¼ã‚¹ãƒ‡ãƒ¼ã‚¿
        race_data_elem = soup.find('div', class_='RaceData01')
        race_data_text = race_data_elem.text.strip() if race_data_elem else ''
        
        # è·é›¢ãƒ»ãƒˆãƒ©ãƒƒã‚¯ç¨®åˆ¥ãªã©ã‚’æŠ½å‡º
        distance = None
        track_type = ''
        weather = ''
        field_condition = ''
        
        if race_data_text:
            import re
            # è·é›¢ï¼ˆä¾‹: èŠ1600mï¼‰
            dist_match = re.search(r'(\d+)m', race_data_text)
            if dist_match:
                distance = int(dist_match.group(1))
            
            # ãƒˆãƒ©ãƒƒã‚¯ç¨®åˆ¥
            if 'èŠ' in race_data_text:
                track_type = 'èŠ'
            elif 'ãƒ€ãƒ¼ãƒˆ' in race_data_text or 'ãƒ€' in race_data_text:
                track_type = 'ãƒ€ãƒ¼ãƒˆ'
            
            # å¤©å€™
            weather_match = re.search(r'å¤©å€™:([^/\s]+)', race_data_text)
            if weather_match:
                weather = weather_match.group(1).strip()
            
            # é¦¬å ´çŠ¶æ…‹
            field_match = re.search(r'é¦¬å ´:([^/\s]+)', race_data_text)
            if field_match:
                field_condition = field_match.group(1).strip()
        
        # çµæœãƒ†ãƒ¼ãƒ–ãƒ«
        results = []
        result_table = soup.find('table', class_='Result_Table')
        if result_table:
            rows = result_table.find_all('tr')
            for row in rows[1:]:  # ãƒ˜ãƒƒãƒ€ãƒ¼è¡Œã‚’ã‚¹ã‚­ãƒƒãƒ—
                cols = row.find_all('td')
                if len(cols) >= 10:
                    result = {
                        'finish_position': cols[0].text.strip(),
                        'bracket_number': cols[1].text.strip(),
                        'horse_number': cols[2].text.strip(),
                        'horse_name': cols[3].text.strip(),
                        'sex_age': cols[4].text.strip(),
                        'jockey_weight': cols[5].text.strip(),
                        'jockey_name': cols[6].text.strip(),
                        'finish_time': cols[7].text.strip(),
                        'margin': cols[8].text.strip(),
                        'odds': cols[9].text.strip(),
                    }
                    results.append(result)
        
        print(f"âœ“ çµæœ: {len(results)}é ­åˆ†ã®ãƒ‡ãƒ¼ã‚¿")
        
        # æ‰•ã„æˆ»ã—ãƒ†ãƒ¼ãƒ–ãƒ«
        payouts = []
        payout_table = soup.find('table', class_='Payout_Detail_Table')
        if payout_table:
            rows = payout_table.find_all('tr')
            for row in rows:
                cols = row.find_all('td')
                if len(cols) >= 3:
                    payout = {
                        'type': cols[0].text.strip(),
                        'numbers': cols[1].text.strip(),
                        'amount': cols[2].text.strip(),
                    }
                    payouts.append(payout)
        
        print(f"âœ“ æ‰•ã„æˆ»ã—: {len(payouts)}ä»¶")
        
        return ScrapeResponse(
            success=True,
            race_name=race_name,
            race_data=race_data_text,
            distance=distance,
            track_type=track_type,
            weather=weather,
            field_condition=field_condition,
            results=results,
            payouts=payouts,
            wait_time=wait_time
        )
        
    except Exception as e:
        print(f"âœ— ã‚¨ãƒ©ãƒ¼: {type(e).__name__}: {str(e)}")
        return ScrapeResponse(success=False, error=str(e), wait_time=wait_time)


@app.get("/health")
def health_check():
    """ãƒ˜ãƒ«ã‚¹ãƒã‚§ãƒƒã‚¯"""
    return {
        "status": "ok",
        "request_count": rate_limiter.request_count,
        "uptime_seconds": (datetime.now() - rate_limiter.start_time).total_seconds()
    }


@app.get("/stats")
def get_stats():
    """çµ±è¨ˆæƒ…å ±"""
    total_elapsed = (datetime.now() - rate_limiter.start_time).total_seconds()
    avg_interval = total_elapsed / rate_limiter.request_count if rate_limiter.request_count > 0 else 0
    
    return {
        "total_requests": rate_limiter.request_count,
        "uptime_seconds": total_elapsed,
        "average_interval_seconds": avg_interval,
        "rate_limit_config": {
            "min_interval": rate_limiter.min_interval,
            "max_interval": rate_limiter.max_interval
        }
    }


if __name__ == '__main__':
    import uvicorn
    print("=" * 80)
    print("ãƒ¬ãƒ¼ãƒˆåˆ¶é™æ©Ÿèƒ½ä»˜ãã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã‚µãƒ¼ãƒ“ã‚¹èµ·å‹•")
    print("=" * 80)
    print(f"æœ€å°é–“éš”: {rate_limiter.min_interval}ç§’")
    print(f"æœ€å¤§é–“éš”: {rate_limiter.max_interval}ç§’")
    print("=" * 80)
    uvicorn.run(app, host='0.0.0.0', port=8001)
