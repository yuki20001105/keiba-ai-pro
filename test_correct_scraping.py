"""
å‚è€ƒã‚µã‚¤ãƒˆã«åŸºã¥ã„ãŸæ­£ã—ã„ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°æ–¹æ³•ã‚’ãƒ†ã‚¹ãƒˆ

é‡è¦ãªç™ºè¦‹:
1. race_idã¯12æ¡: 202401060101 (YYYYMMDD + å ´ã‚³ãƒ¼ãƒ‰2æ¡ + ãƒ¬ãƒ¼ã‚¹ç•ªå·2æ¡)
   â€»å¾“æ¥ã®14æ¡ã§ã¯ãªãã€é–‹å‚¬å›ãƒ»é–‹å‚¬æ—¥æƒ…å ±ã¯å«ã¾ãªã„
   
2. é–‹å‚¬æ—¥ä¸€è¦§ã¯ã‚«ãƒ¬ãƒ³ãƒ€ãƒ¼ãƒšãƒ¼ã‚¸ã‹ã‚‰å–å¾—
3. race_idä¸€è¦§ã¯å„é–‹å‚¬æ—¥ã®race_list.htmlã‹ã‚‰å–å¾—
"""
import requests
from bs4 import BeautifulSoup
import re

def test_calendar_scraping():
    """ã‚«ãƒ¬ãƒ³ãƒ€ãƒ¼ãƒšãƒ¼ã‚¸ã‹ã‚‰é–‹å‚¬æ—¥ã‚’å–å¾—"""
    print("=" * 80)
    print("1. ã‚«ãƒ¬ãƒ³ãƒ€ãƒ¼ãƒšãƒ¼ã‚¸ã‹ã‚‰é–‹å‚¬æ—¥ã‚’å–å¾—")
    print("=" * 80)
    
    url = "https://race.netkeiba.com/top/calendar.html?year=2024&month=1"
    headers = {'User-Agent': 'Mozilla/5.0'}
    
    response = requests.get(url, headers=headers, timeout=10)
    html = response.text
    
    # kaisai_date=YYYYMMDDã®ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’æŠ½å‡º
    dates = re.findall(r'kaisai_date=(\d{8})', html)
    unique_dates = sorted(set(dates))
    
    print(f"Status: {response.status_code}")
    print(f"Found {len(unique_dates)} unique dates:")
    for date in unique_dates[:10]:
        print(f"  {date}")
    
    return unique_dates

def test_race_list_scraping(kaisai_date):
    """race_list.htmlã‹ã‚‰12æ¡ã®race_idã‚’å–å¾—"""
    print(f"\n{'=' * 80}")
    print(f"2. race_list.htmlã‹ã‚‰race_idä¸€è¦§ã‚’å–å¾— (kaisai_date={kaisai_date})")
    print("=" * 80)
    
    url = f"https://race.netkeiba.com/top/race_list.html?kaisai_date={kaisai_date}"
    headers = {'User-Agent': 'Mozilla/5.0'}
    
    response = requests.get(url, headers=headers, timeout=10)
    html = response.text
    soup = BeautifulSoup(html, 'html.parser')
    
    # race_id=æ•°å­— ã®ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’æŠ½å‡º
    race_ids = []
    
    # ãƒªãƒ³ã‚¯ã‹ã‚‰race_idã‚’æŠ½å‡º
    for link in soup.find_all('a', href=True):
        href = link['href']
        match = re.search(r'race_id=(\d{12})', href)
        if match:
            race_id = match.group(1)
            if race_id not in race_ids:
                race_ids.append(race_id)
    
    print(f"Status: {response.status_code}")
    print(f"Found {len(race_ids)} race IDs:")
    for i, race_id in enumerate(race_ids[:15]):
        print(f"  {i+1}. {race_id}")
    
    return race_ids

def test_race_scraping(race_id):
    """å®Ÿéš›ã®ãƒ¬ãƒ¼ã‚¹çµæœãƒšãƒ¼ã‚¸ã‹ã‚‰ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—"""
    print(f"\n{'=' * 80}")
    print(f"3. ãƒ¬ãƒ¼ã‚¹çµæœãƒšãƒ¼ã‚¸ã‹ã‚‰ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾— (race_id={race_id})")
    print("=" * 80)
    
    url = f"https://race.netkeiba.com/race/result.html?race_id={race_id}"
    headers = {'User-Agent': 'Mozilla/5.0'}
    
    response = requests.get(url, headers=headers, timeout=10)
    html = response.text
    soup = BeautifulSoup(html, 'html.parser')
    
    print(f"Status: {response.status_code}")
    
    # ãƒ¬ãƒ¼ã‚¹åã‚’å–å¾—
    race_name = soup.find('div', class_='RaceName')
    if race_name:
        print(f"Race Name: {race_name.get_text(strip=True)}")
    
    # ãƒ¬ãƒ¼ã‚¹æƒ…å ±ã‚’å–å¾—
    race_data = soup.find('div', class_='RaceData01')
    if race_data:
        print(f"Race Data: {race_data.get_text(strip=True)[:100]}")
    
    # çµæœãƒ†ãƒ¼ãƒ–ãƒ«ã‚’å–å¾—
    result_table = soup.find('table', class_='Race_Result_Table')
    if result_table:
        rows = result_table.find_all('tr')
        print(f"Result Table: {len(rows)-1} horses found")
        
        # æœ€åˆã®3é ­ã‚’è¡¨ç¤º
        for i, row in enumerate(rows[1:4]):
            cols = row.find_all('td')
            if len(cols) >= 3:
                finish = cols[0].get_text(strip=True)
                horse = cols[3].get_text(strip=True) if len(cols) > 3 else 'N/A'
                print(f"  {finish}ç€: {horse}")
        
        return True
    else:
        print("Result Table: NOT FOUND")
        return False

if __name__ == "__main__":
    print("\nğŸš€ æ­£ã—ã„ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°æ–¹æ³•ã®ãƒ†ã‚¹ãƒˆé–‹å§‹\n")
    
    # 1. ã‚«ãƒ¬ãƒ³ãƒ€ãƒ¼ã‹ã‚‰é–‹å‚¬æ—¥ã‚’å–å¾—
    dates = test_calendar_scraping()
    
    if dates:
        # 2. æœ€åˆã®é–‹å‚¬æ—¥ã®race_idä¸€è¦§ã‚’å–å¾—
        first_date = dates[0]
        race_ids = test_race_list_scraping(first_date)
        
        if race_ids:
            # 3. æœ€åˆã®ãƒ¬ãƒ¼ã‚¹ã‚’ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°
            first_race_id = race_ids[0]
            success = test_race_scraping(first_race_id)
            
            print("\n" + "=" * 80)
            print("ãƒ†ã‚¹ãƒˆçµæœã‚µãƒãƒªãƒ¼")
            print("=" * 80)
            print(f"âœ… é–‹å‚¬æ—¥å–å¾—: {len(dates)}æ—¥")
            print(f"âœ… race_idå–å¾—: {len(race_ids)}ãƒ¬ãƒ¼ã‚¹")
            print(f"{'âœ…' if success else 'âŒ'} ãƒ¬ãƒ¼ã‚¹è©³ç´°å–å¾—: {'æˆåŠŸ' if success else 'å¤±æ•—'}")
            
            if success:
                print("\nğŸ‰ æ­£ã—ã„æ–¹æ³•ã§ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ãŒæˆåŠŸã—ã¾ã—ãŸï¼")
                print(f"\nğŸ’¡ é‡è¦: race_idã¯12æ¡ (ä¾‹: {first_race_id})")
                print("   å½¢å¼: YYYYMMDD + å ´ã‚³ãƒ¼ãƒ‰2æ¡ + ãƒ¬ãƒ¼ã‚¹ç•ªå·2æ¡")
        else:
            print("\nâŒ race_idä¸€è¦§ã®å–å¾—ã«å¤±æ•—")
    else:
        print("\nâŒ é–‹å‚¬æ—¥ä¸€è¦§ã®å–å¾—ã«å¤±æ•—")
