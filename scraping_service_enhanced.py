"""
å…¨ç‰¹å¾´é‡å–å¾—å¯¾å¿œ - æ‹¡å¼µç‰ˆã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã‚µãƒ¼ãƒ“ã‚¹
- é¦¬è©³ç´°ï¼ˆè¡€çµ±ã€éå»æˆç¸¾ï¼‰
- é¨æ‰‹è©³ç´°ï¼ˆå‹ç‡ã€é€£å¯¾ç‡ã€è¤‡å‹ç‡ï¼‰
- èª¿æ•™å¸«è©³ç´°ï¼ˆå‹ç‡ç­‰ï¼‰
- ãƒ©ãƒƒãƒ—ã‚¿ã‚¤ãƒ ã€ã‚³ãƒ¼ãƒŠãƒ¼é€šéé †ä½
- çµæœãƒ†ãƒ¼ãƒ–ãƒ«å…¨15åˆ—
"""
from fastapi import FastAPI, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
import undetected_chromedriver as uc
from bs4 import BeautifulSoup
import requests
import time
import random
from datetime import datetime
from typing import Optional
import threading
import re

app = FastAPI()

# CORSè¨­å®š
app.add_middleware(
    CORSMiddleware,
    allow_origins=["http://localhost:3000"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# ãƒ¬ãƒ¼ãƒˆåˆ¶é™ç®¡ç†
class RateLimiter:
    def __init__(self, min_interval=3.0, max_interval=7.0):
        self.min_interval = min_interval
        self.max_interval = max_interval
        self.last_request_time: Optional[datetime] = None
        self.request_count = 0
        self.start_time = datetime.now()
        self.lock = threading.Lock()
    
    def wait_if_needed(self):
        """å¿…è¦ã«å¿œã˜ã¦å¾…æ©Ÿ"""
        with self.lock:
            if self.last_request_time is None:
                self.last_request_time = datetime.now()
                return 0
            
            elapsed = (datetime.now() - self.last_request_time).total_seconds()
            required_wait = random.uniform(self.min_interval, self.max_interval)
            
            if elapsed < required_wait:
                wait_time = required_wait - elapsed
                print(f"â° ãƒ¬ãƒ¼ãƒˆåˆ¶é™: {wait_time:.1f}ç§’å¾…æ©Ÿã—ã¾ã™...")
                time.sleep(wait_time)
            else:
                wait_time = 0
            
            self.last_request_time = datetime.now()
            self.request_count += 1
            
            return wait_time

# ã‚°ãƒ­ãƒ¼ãƒãƒ«ãªãƒ¬ãƒ¼ãƒˆåˆ¶é™ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹
rate_limiter = RateLimiter(min_interval=3.0, max_interval=7.0)

# ã‚°ãƒ­ãƒ¼ãƒãƒ«ãªChromeãƒ‰ãƒ©ã‚¤ãƒãƒ¼
_driver: Optional[uc.Chrome] = None
_driver_lock = threading.Lock()

def get_driver():
    """Chromeãƒ‰ãƒ©ã‚¤ãƒãƒ¼ã‚’å–å¾—ï¼ˆã‚·ãƒ³ã‚°ãƒ«ãƒˆãƒ³ãƒ‘ã‚¿ãƒ¼ãƒ³ï¼‰"""
    global _driver
    with _driver_lock:
        if _driver is None:
            print("ğŸš€ Chrome WebDriveråˆæœŸåŒ–ä¸­...")
            options = uc.ChromeOptions()
            options.headless = False
            options.add_argument('--no-sandbox')
            options.add_argument('--disable-dev-shm-usage')
            _driver = uc.Chrome(options=options, use_subprocess=False, version_main=None)
            print("âœ“ Chrome WebDriveråˆæœŸåŒ–å®Œäº†")
        return _driver


def scrape_horse_details(horse_url: str):
    """é¦¬è©³ç´°ãƒšãƒ¼ã‚¸ã‹ã‚‰è¡€çµ±æƒ…å ±ã¨éå»æˆç¸¾ã‚’å–å¾—"""
    try:
        driver = get_driver()
        full_url = f'https://db.netkeiba.com{horse_url}' if horse_url.startswith('/') else horse_url
        
        print(f"  â†’ é¦¬è©³ç´°å–å¾—: {full_url}")
        driver.get(full_url)
        time.sleep(random.uniform(1.5, 2.5))
        
        soup = BeautifulSoup(driver.page_source, 'html.parser')
        
        details = {}
        
        # åŸºæœ¬æƒ…å ±ãƒ†ãƒ¼ãƒ–ãƒ«
        profile_table = soup.find('table', class_='db_prof_table')
        if profile_table:
            rows = profile_table.find_all('tr')
            for row in rows:
                th = row.find('th')
                td = row.find('td')
                if th and td:
                    key = th.text.strip()
                    value = td.text.strip()
                    
                    if 'ç”Ÿå¹´æœˆæ—¥' in key:
                        details['birth_date'] = value
                    elif 'èª¿æ•™å¸«' in key:
                        details['trainer'] = value
                    elif 'é¦¬ä¸»' in key:
                        details['owner'] = value
                    elif 'ç”Ÿç”£è€…' in key:
                        details['breeder'] = value
                    elif 'ç”£åœ°' in key:
                        details['breeding_farm'] = value
        
        # è¡€çµ±æƒ…å ±
        pedigree_table = soup.find('table', class_='blood_table')
        if pedigree_table:
            # çˆ¶é¦¬
            sire = pedigree_table.find('a', href=re.compile(r'/horse/'))
            if sire:
                details['sire'] = sire.text.strip()
            
            # æ¯é¦¬ãƒ»æ¯çˆ¶é¦¬ã‚‚åŒæ§˜ã«å–å¾—å¯èƒ½
            all_horses = pedigree_table.find_all('a', href=re.compile(r'/horse/'))
            if len(all_horses) >= 2:
                details['dam'] = all_horses[1].text.strip()
            if len(all_horses) >= 3:
                details['damsire'] = all_horses[2].text.strip()
        
        # éå»æˆç¸¾ã‚µãƒãƒªãƒ¼
        record_table = soup.find('table', class_='db_h_race_results')
        if record_table:
            rows = record_table.find_all('tr')[1:6]  # æœ€æ–°5ãƒ¬ãƒ¼ã‚¹
            past_performances = []
            for row in rows:
                cols = row.find_all('td')
                if len(cols) >= 12:
                    perf = {
                        'date': cols[0].text.strip(),
                        'venue': cols[1].text.strip(),
                        'race_name': cols[4].text.strip(),
                        'finish': cols[11].text.strip(),
                        'jockey': cols[12].text.strip() if len(cols) > 12 else '',
                    }
                    past_performances.append(perf)
            details['past_performances'] = past_performances
        
        print(f"    âœ“ é¦¬è©³ç´°å–å¾—å®Œäº†: {len(details)}é …ç›®")
        return details
        
    except Exception as e:
        print(f"    âœ— é¦¬è©³ç´°å–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
        return {}


def scrape_jockey_details(jockey_url: str):
    """é¨æ‰‹è©³ç´°ãƒšãƒ¼ã‚¸ã‹ã‚‰å‹ç‡ç­‰ã®çµ±è¨ˆæƒ…å ±ã‚’å–å¾—"""
    try:
        driver = get_driver()
        full_url = f'https://db.netkeiba.com{jockey_url}' if jockey_url.startswith('/') else jockey_url
        
        print(f"  â†’ é¨æ‰‹è©³ç´°å–å¾—: {full_url}")
        driver.get(full_url)
        time.sleep(random.uniform(1.5, 2.5))
        
        soup = BeautifulSoup(driver.page_source, 'html.parser')
        
        details = {}
        
        # ãƒ‡ãƒ¼ã‚¿åˆ†æãƒ†ãƒ¼ãƒ–ãƒ« - é€šç®—æˆç¸¾
        data_table = soup.find('table', class_='nk_tb_common')
        if data_table:
            # ãƒ˜ãƒƒãƒ€ãƒ¼ã¨ãƒ‡ãƒ¼ã‚¿è¡Œã‚’æ¢ã™
            headers = data_table.find('thead')
            body = data_table.find('tbody')
            
            if headers and body:
                header_cols = [th.text.strip() for th in headers.find_all('th')]
                data_rows = body.find_all('tr')
                
                # é€šç®—æˆç¸¾ã®è¡Œã‚’æ¢ã™
                for row in data_rows:
                    cols = row.find_all('td')
                    if cols and 'é€šç®—' in cols[0].text:
                        # å‹ç‡ã€é€£å¯¾ç‡ã€è¤‡å‹ç‡ã‚’å–å¾—
                        for i, header in enumerate(header_cols):
                            if i < len(cols):
                                value = cols[i].text.strip()
                                if 'å‹ç‡' in header:
                                    try:
                                        details['win_rate'] = float(value.replace('%', ''))
                                    except:
                                        pass
                                elif 'é€£å¯¾ç‡' in header:
                                    try:
                                        details['place_rate_top2'] = float(value.replace('%', ''))
                                    except:
                                        pass
                                elif 'è¤‡å‹ç‡' in header:
                                    try:
                                        details['show_rate'] = float(value.replace('%', ''))
                                    except:
                                        pass
        
        # é€šç®—æˆç¸¾ãŒå–ã‚Œãªã‹ã£ãŸå ´åˆã€å…¨ãƒšãƒ¼ã‚¸ã®ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰æŠ½å‡º
        if not details:
            page_text = soup.get_text()
            win_match = re.search(r'å‹ç‡[\s:ï¼š]*([0-9.]+)%', page_text)
            if win_match:
                details['win_rate'] = float(win_match.group(1))
            
            place_match = re.search(r'é€£å¯¾ç‡[\s:ï¼š]*([0-9.]+)%', page_text)
            if place_match:
                details['place_rate_top2'] = float(place_match.group(1))
            
            show_match = re.search(r'è¤‡å‹ç‡[\s:ï¼š]*([0-9.]+)%', page_text)
            if show_match:
                details['show_rate'] = float(show_match.group(1))
        
        print(f"    âœ“ é¨æ‰‹è©³ç´°å–å¾—å®Œäº†: {len(details)}é …ç›®")
        return details
        
    except Exception as e:
        print(f"    âœ— é¨æ‰‹è©³ç´°å–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
        return {}


def scrape_trainer_details(trainer_url: str):
    """èª¿æ•™å¸«è©³ç´°ãƒšãƒ¼ã‚¸ã‹ã‚‰çµ±è¨ˆæƒ…å ±ã‚’å–å¾—"""
    try:
        driver = get_driver()
        full_url = f'https://db.netkeiba.com{trainer_url}' if trainer_url.startswith('/') else trainer_url
        
        print(f"  â†’ èª¿æ•™å¸«è©³ç´°å–å¾—: {full_url}")
        driver.get(full_url)
        time.sleep(random.uniform(1.5, 2.5))
        
        soup = BeautifulSoup(driver.page_source, 'html.parser')
        
        details = {}
        
        # ãƒ‡ãƒ¼ã‚¿åˆ†æãƒ†ãƒ¼ãƒ–ãƒ« - é€šç®—æˆç¸¾
        data_table = soup.find('table', class_='nk_tb_common')
        if data_table:
            headers = data_table.find('thead')
            body = data_table.find('tbody')
            
            if headers and body:
                header_cols = [th.text.strip() for th in headers.find_all('th')]
                data_rows = body.find_all('tr')
                
                # é€šç®—æˆç¸¾ã®è¡Œã‚’æ¢ã™
                for row in data_rows:
                    cols = row.find_all('td')
                    if cols and 'é€šç®—' in cols[0].text:
                        for i, header in enumerate(header_cols):
                            if i < len(cols):
                                value = cols[i].text.strip()
                                if 'å‹ç‡' in header:
                                    try:
                                        details['win_rate'] = float(value.replace('%', ''))
                                    except:
                                        pass
                                elif 'é€£å¯¾ç‡' in header:
                                    try:
                                        details['place_rate_top2'] = float(value.replace('%', ''))
                                    except:
                                        pass
                                elif 'è¤‡å‹ç‡' in header:
                                    try:
                                        details['show_rate'] = float(value.replace('%', ''))
                                    except:
                                        pass
        
        # ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰æŠ½å‡º
        if not details:
            page_text = soup.get_text()
            win_match = re.search(r'å‹ç‡[\s:ï¼š]*([0-9.]+)%', page_text)
            if win_match:
                details['win_rate'] = float(win_match.group(1))
            
            place_match = re.search(r'é€£å¯¾ç‡[\s:ï¼š]*([0-9.]+)%', page_text)
            if place_match:
                details['place_rate_top2'] = float(place_match.group(1))
        
        print(f"    âœ“ èª¿æ•™å¸«è©³ç´°å–å¾—å®Œäº†: {len(details)}é …ç›®")
        return details
        
    except Exception as e:
        print(f"    âœ— èª¿æ•™å¸«è©³ç´°å–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
        return {}


class EnhancedScrapeRequest(BaseModel):
    race_id: str
    include_details: bool = True  # è©³ç´°ãƒšãƒ¼ã‚¸ã‚‚å–å¾—ã™ã‚‹ã‹

class EnhancedScrapeResponse(BaseModel):
    success: bool
    race_info: dict = {}
    results: list[dict] = []
    lap_times: dict = {}
    corner_positions: dict = {}
    payouts: list[dict] = []
    error: str | None = None


@app.post("/scrape/enhanced", response_model=EnhancedScrapeResponse)
def scrape_race_enhanced(request: EnhancedScrapeRequest):
    """
    å…¨ç‰¹å¾´é‡ã‚’å–å¾—ã™ã‚‹æ‹¡å¼µç‰ˆã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°
    """
    wait_time = rate_limiter.wait_if_needed()
    
    race_id = request.race_id
    url = f'https://race.netkeiba.com/race/result.html?race_id={race_id}'
    
    try:
        driver = get_driver()
        
        print(f"â†’ ãƒ¬ãƒ¼ã‚¹çµæœãƒšãƒ¼ã‚¸å–å¾—: {url}")
        driver.get(url)
        time.sleep(random.uniform(2.0, 3.0))
        
        soup = BeautifulSoup(driver.page_source, 'html.parser')
        
        # ===== ãƒ¬ãƒ¼ã‚¹åŸºæœ¬æƒ…å ± =====
        race_info = {}
        
        # ãƒ¬ãƒ¼ã‚¹å
        race_name_elem = soup.find('h1', class_='RaceName')
        if race_name_elem:
            race_info['race_name'] = race_name_elem.text.strip()
        
        # RaceData01
        data01 = soup.find('div', class_='RaceData01')
        if data01:
            text = data01.text.strip()
            race_info['race_data_01'] = text
            
            # ç™ºèµ°æ™‚åˆ»
            time_match = re.search(r'(\d+:\d+)ç™ºèµ°', text)
            if time_match:
                race_info['post_time'] = time_match.group(1)
            
            # ãƒˆãƒ©ãƒƒã‚¯ç¨®åˆ¥
            if 'èŠ' in text:
                race_info['track_type'] = 'èŠ'
            elif 'ãƒ€ãƒ¼ãƒˆ' in text or 'ãƒ€' in text:
                race_info['track_type'] = 'ãƒ€ãƒ¼ãƒˆ'
            
            # è·é›¢
            dist_match = re.search(r'(\d+)m', text)
            if dist_match:
                race_info['distance'] = int(dist_match.group(1))
            
            # ã‚³ãƒ¼ã‚¹æ–¹å‘
            if 'å³' in text:
                race_info['course_direction'] = 'å³'
            elif 'å·¦' in text:
                race_info['course_direction'] = 'å·¦'
            
            # å¤©å€™
            weather_match = re.search(r'å¤©å€™:([^\s/]+)', text)
            if weather_match:
                race_info['weather'] = weather_match.group(1)
            
            # é¦¬å ´çŠ¶æ…‹
            field_match = re.search(r'é¦¬å ´:([^\s]+)', text)
            if field_match:
                race_info['field_condition'] = field_match.group(1)
        
        # RaceData02
        data02 = soup.find('div', class_='RaceData02')
        if data02:
            text = data02.text.strip()
            race_info['race_data_02'] = text
            
            # é–‹å‚¬æƒ…å ±
            kaisai_match = re.search(r'(\d+)å›\s+([^\s]+)\s+(\d+)æ—¥ç›®', text)
            if kaisai_match:
                race_info['kai'] = int(kaisai_match.group(1))
                race_info['venue'] = kaisai_match.group(2)
                race_info['day'] = int(kaisai_match.group(3))
            
            # ãƒ¬ãƒ¼ã‚¹ã‚¯ãƒ©ã‚¹
            for cls in ['ã‚ªãƒ¼ãƒ—ãƒ³', 'æ–°é¦¬', 'æœªå‹åˆ©', 'ï¼‘å‹ã‚¯ãƒ©ã‚¹', '1å‹ã‚¯ãƒ©ã‚¹', 'ï¼’å‹ã‚¯ãƒ©ã‚¹', '2å‹ã‚¯ãƒ©ã‚¹', 'ï¼“å‹ã‚¯ãƒ©ã‚¹', '3å‹ã‚¯ãƒ©ã‚¹']:
                if cls in text:
                    race_info['race_class'] = cls
                    break
            
            # å‡ºèµ°é ­æ•°
            head_match = re.search(r'(\d+)é ­', text)
            if head_match:
                race_info['horse_count'] = int(head_match.group(1))
        
        # è³é‡‘
        prize_elem = soup.find(string=re.compile('æœ¬è³é‡‘'))
        if prize_elem:
            race_info['prize_money'] = prize_elem.strip()
        
        print(f"âœ“ ãƒ¬ãƒ¼ã‚¹åŸºæœ¬æƒ…å ±: {len(race_info)}é …ç›®")
        
        # ===== çµæœãƒ†ãƒ¼ãƒ–ãƒ«ï¼ˆå…¨15åˆ—ï¼‰ =====
        results = []
        result_table = soup.find('table', id='All_Result_Table')
        
        if not result_table:
            # idãŒãªã„å ´åˆã€å†…å®¹ã‹ã‚‰æ¤œç´¢
            tables = soup.find_all('table')
            for table in tables:
                if 'ç€é †' in table.text and 'é¦¬å' in table.text:
                    result_table = table
                    break
        
        if result_table:
            rows = result_table.find_all('tr')[1:]  # ãƒ˜ãƒƒãƒ€ãƒ¼é™¤ã
            
            for row in rows:
                cols = row.find_all('td')
                if len(cols) >= 15:
                    horse_data = {
                        'finish_position': cols[0].text.strip(),
                        'bracket_number': cols[1].text.strip(),
                        'horse_number': cols[2].text.strip(),
                    }
                    
                    # é¦¬åï¼ˆãƒªãƒ³ã‚¯ï¼‰
                    horse_link = cols[3].find('a')
                    if horse_link:
                        horse_data['horse_name'] = horse_link.text.strip()
                        horse_data['horse_url'] = horse_link.get('href', '')
                    else:
                        horse_data['horse_name'] = cols[3].text.strip()
                        horse_data['horse_url'] = ''
                    
                    horse_data['sex_age'] = cols[4].text.strip()
                    horse_data['jockey_weight'] = cols[5].text.strip()
                    
                    # é¨æ‰‹ï¼ˆãƒªãƒ³ã‚¯ï¼‰
                    jockey_link = cols[6].find('a')
                    if jockey_link:
                        horse_data['jockey_name'] = jockey_link.text.strip()
                        horse_data['jockey_url'] = jockey_link.get('href', '')
                    else:
                        horse_data['jockey_name'] = cols[6].text.strip()
                        horse_data['jockey_url'] = ''
                    
                    horse_data['finish_time'] = cols[7].text.strip()
                    horse_data['margin'] = cols[8].text.strip()
                    horse_data['popularity'] = cols[9].text.strip()
                    horse_data['odds'] = cols[10].text.strip()
                    horse_data['last_3f'] = cols[11].text.strip()
                    horse_data['corner_positions'] = cols[12].text.strip()
                    
                    # èª¿æ•™å¸«ï¼ˆãƒªãƒ³ã‚¯ï¼‰
                    trainer_link = cols[13].find('a')
                    if trainer_link:
                        horse_data['trainer_name'] = trainer_link.text.strip()
                        horse_data['trainer_url'] = trainer_link.get('href', '')
                    else:
                        horse_data['trainer_name'] = cols[13].text.strip()
                        horse_data['trainer_url'] = ''
                    
                    horse_data['weight'] = cols[14].text.strip()
                    
                    # è©³ç´°ãƒšãƒ¼ã‚¸ã‚‚å–å¾—ã™ã‚‹å ´åˆ
                    if request.include_details:
                        if horse_data.get('horse_url'):
                            horse_details = scrape_horse_details(horse_data['horse_url'])
                            horse_data['horse_details'] = horse_details
                        
                        if horse_data.get('jockey_url'):
                            jockey_details = scrape_jockey_details(horse_data['jockey_url'])
                            horse_data['jockey_details'] = jockey_details
                        
                        if horse_data.get('trainer_url'):
                            trainer_details = scrape_trainer_details(horse_data['trainer_url'])
                            horse_data['trainer_details'] = trainer_details
                    
                    results.append(horse_data)
            
            print(f"âœ“ çµæœãƒ†ãƒ¼ãƒ–ãƒ«: {len(results)}é ­")
        
        # ===== ãƒ©ãƒƒãƒ—ã‚¿ã‚¤ãƒ  =====
        lap_times = {}
        lap_table = soup.find('table', class_='Race_HaronTime')
        if lap_table:
            headers = lap_table.find('tr')
            if headers:
                distances = [th.text.strip() for th in headers.find_all(['th', 'td'])]
                times_row = lap_table.find_all('tr')[1] if len(lap_table.find_all('tr')) > 1 else None
                if times_row:
                    times = [td.text.strip() for td in times_row.find_all('td')]
                    for dist, t in zip(distances, times):
                        lap_times[dist] = t
            print(f"âœ“ ãƒ©ãƒƒãƒ—ã‚¿ã‚¤ãƒ : {len(lap_times)}åœ°ç‚¹")
        
        # ===== ã‚³ãƒ¼ãƒŠãƒ¼é€šéé †ä½ =====
        corner_positions = {}
        corner_table = soup.find('table', class_='Corner_Num')
        if corner_table:
            rows = corner_table.find_all('tr')
            for row in rows:
                cols = row.find_all(['th', 'td'])
                if len(cols) >= 2:
                    corner = cols[0].text.strip()
                    order = cols[1].text.strip()
                    if corner and order:
                        corner_positions[corner] = order
            print(f"âœ“ ã‚³ãƒ¼ãƒŠãƒ¼é€šé: {len(corner_positions)}åœ°ç‚¹")
        
        # ===== æ‰•æˆ» =====
        payouts = []
        payout_tables = soup.find_all('table', class_='Payout_Detail_Table')
        for table in payout_tables:
            rows = table.find_all('tr')
            for row in rows:
                cols = row.find_all(['th', 'td'])
                if len(cols) >= 3:
                    payout = {
                        'type': cols[0].text.strip(),
                        'numbers': cols[1].text.strip(),
                        'amount': cols[2].text.strip(),
                    }
                    payouts.append(payout)
        print(f"âœ“ æ‰•æˆ»: {len(payouts)}ä»¶")
        
        return EnhancedScrapeResponse(
            success=True,
            race_info=race_info,
            results=results,
            lap_times=lap_times,
            corner_positions=corner_positions,
            payouts=payouts
        )
        
    except Exception as e:
        print(f"âœ— ã‚¨ãƒ©ãƒ¼: {e}")
        import traceback
        traceback.print_exc()
        return EnhancedScrapeResponse(success=False, error=str(e))


class RaceListRequest(BaseModel):
    kaisai_date: str  # YYYYMMDDå½¢å¼

class RaceListResponse(BaseModel):
    success: bool
    race_ids: list[str] = []
    error: str | None = None


@app.post("/race_list", response_model=RaceListResponse)
def get_race_list(request: RaceListRequest):
    """æŒ‡å®šæ—¥ã®race_idä¸€è¦§ã‚’å–å¾—"""
    kaisai_date = request.kaisai_date
    url = f'https://race.netkeiba.com/top/race_list.html?kaisai_date={kaisai_date}'
    
    print(f"ğŸ“… {kaisai_date[:4]}å¹´{kaisai_date[4:6]}æœˆ{kaisai_date[6:8]}æ—¥ã®ãƒ¬ãƒ¼ã‚¹ä¸€è¦§å–å¾—ä¸­...")
    
    try:
        driver = get_driver()
        driver.get(url)
        time.sleep(random.uniform(2.0, 3.0))
        
        soup = BeautifulSoup(driver.page_source, 'html.parser')
        
        race_ids = []
        for link in soup.find_all('a', href=True):
            href = link['href']
            match = re.search(r'race_id=(\d{12})', href)
            if match:
                race_id = match.group(1)
                if race_id not in race_ids:
                    race_ids.append(race_id)
        
        print(f"âœ“ {len(race_ids)}ä»¶ã®ãƒ¬ãƒ¼ã‚¹ã‚’å–å¾—")
        
        return RaceListResponse(success=True, race_ids=race_ids)
        
    except Exception as e:
        print(f"âœ— ã‚¨ãƒ©ãƒ¼: {e}")
        return RaceListResponse(success=False, error=str(e))


@app.get("/health")
def health_check():
    """ãƒ˜ãƒ«ã‚¹ãƒã‚§ãƒƒã‚¯"""
    return {
        "status": "ok",
        "request_count": rate_limiter.request_count,
        "uptime_seconds": (datetime.now() - rate_limiter.start_time).total_seconds(),
        "driver_initialized": _driver is not None
    }


@app.on_event("shutdown")
def shutdown_event():
    """ã‚·ãƒ£ãƒƒãƒˆãƒ€ã‚¦ãƒ³æ™‚ã«ãƒ‰ãƒ©ã‚¤ãƒãƒ¼ã‚’ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—"""
    global _driver
    if _driver is not None:
        print("ğŸ›‘ Chrome WebDriverã‚’ã‚¯ãƒ­ãƒ¼ã‚ºã—ã¾ã™...")
        _driver.quit()
        _driver = None


if __name__ == '__main__':
    import uvicorn
    print("=" * 80)
    print("å…¨ç‰¹å¾´é‡å–å¾—å¯¾å¿œ æ‹¡å¼µç‰ˆã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã‚µãƒ¼ãƒ“ã‚¹èµ·å‹•")
    print("=" * 80)
    print("æ©Ÿèƒ½:")
    print("  - ãƒ¬ãƒ¼ã‚¹åŸºæœ¬æƒ…å ±ï¼ˆ15é …ç›®ä»¥ä¸Šï¼‰")
    print("  - çµæœãƒ†ãƒ¼ãƒ–ãƒ«å…¨15åˆ—")
    print("  - é¦¬è©³ç´°ï¼ˆè¡€çµ±ã€éå»æˆç¸¾ï¼‰")
    print("  - é¨æ‰‹è©³ç´°ï¼ˆå‹ç‡ã€é€£å¯¾ç‡ã€è¤‡å‹ç‡ï¼‰")
    print("  - èª¿æ•™å¸«è©³ç´°ï¼ˆå‹ç‡ç­‰ï¼‰")
    print("  - ãƒ©ãƒƒãƒ—ã‚¿ã‚¤ãƒ ")
    print("  - ã‚³ãƒ¼ãƒŠãƒ¼é€šéé †ä½")
    print("=" * 80)
    uvicorn.run(app, host='0.0.0.0', port=8001)
